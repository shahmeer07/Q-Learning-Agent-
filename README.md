# ğŸ¤– Q-Learning Agent Solving a Custom GridWorld  
### A Reinforcement Learning Experiment with Jump Tiles, Obstacles & Dynamic Reward Shaping  
**Â© 2025 â€” Shahmeer Khan**

---

![Banner](https://dummyimage.com/1200x260/0b3d91/ffffff&text=Q-Learning+GridWorld+Reinforcement+Learning)

<div align="center">

ğŸ§­ **Custom GridWorld Environment** â€¢ ğŸ¤– **Q-Learning Agent** â€¢ ğŸ“Š **Value Function Visualization**  
**Dynamic Rewards, Jump Tiles, Obstacles & Multiple Learning Rates**

</div>

---

# ğŸ“˜ Overview

This repository contains a complete reinforcement learning experiment where a **Q-Learning agent** learns to navigate a **5Ã—5 GridWorld** with:

âœ” Obstacles  
âœ” A start & goal state  
âœ” Penalty for invalid moves  
âœ” A bonus jump tile (`J â†’ JT`)  
âœ” Positive terminal reward  
âœ” Negative step cost  
âœ” Multiple learning rates (Î±) comparison  
âœ” Policy visualization with `matplotlib`  

The goal is to study how learning rate impacts convergence, optimal paths, Q-values, and training stability.

---

# ğŸ—º GridWorld Layout

### Environment Features

| Element | Description |
|--------|-------------|
| **S** | Start at `(1, 0)` |
| **G** | Goal at `(4, 4)` â€” reward `+10` |
| **J â†’ JT** | Jump tile: stepping on `(1,3)` teleports to `(3,3)` with reward `+5` |
| **X** | Obstacles at `(2,1)` and `(3,1)` |
| **Actions** | Up, Down, Left, Right (4 discrete actions) |
| **Step Reward** | `-1` |
| **Invalid Move** | No movement + penalty `-1` |

---

# ğŸš€ Q-Learning Algorithm

The agent uses the standard Q-Learning update rule:

\[
Q(s,a) \leftarrow Q(s,a) + \alpha \big[r + \gamma \max_a Q(s',a) - Q(s,a)\big]
\]

Hyperparameters:

```python
alpha   # learning rate
gamma   = 0.7   # discount factor
epsilon = 0.1   # exploration rate (decays over time)
```

### ğŸ§  Features Implemented
## âœ” 1. Custom GridWorld Class

Validity checking

Reward shaping

Jump tile

Goal detection

Obstacles

## âœ” 2. Q-Learning Trainer

Îµ-greedy policy

Early stopping

Reward tracking

Q-table learning for multiple Î± values

## âœ” 3. Policy Simulation

Once trained, the agent runs deterministically using:

action = np.argmax(Q[state])


Produces:

Optimal path

Total reward

## âœ” 4. Visualizations

Using matplotlib, the project generates:

ğŸ“ˆ Rewards vs. Episodes
ğŸ“Š Heatmap of state-values
ğŸ§­ Arrows showing optimal policy
ğŸŸ© Highlighting start, goal, obstacles, and jump tiles

### ğŸ“‚ Repository Files
# File	Purpose
gridworld_q_learning.py	Full environment + Q-Learning + visualization script
# ğŸ” Code Summary
GridWorld definition
self.rows = 5
self.cols = 5
self.start = (1, 0)
self.goal = (4, 4)
self.jump_from = (1, 3)
self.jump_to = (3, 3)
self.obstacles = [(2, 1), (3, 1)]

Q-Learning training loop
Q[state][action] += alpha * (
    reward + gamma * max(Q[next_state]) - Q[state][action]
)

Visualizing value & policy
values = np.max(Q, axis=2)
policy  = np.argmax(Q, axis=2)
ax.imshow(values, cmap='viridis')
ax.quiver(...)

ğŸ“Š Example Outputs
ğŸ“ˆ Reward curves for Î± âˆˆ {1.0, 0.5, 0.1}

Observe:

Î± = 1.0 â†’ unstable, high variance

Î± = 0.5 â†’ fast learning, moderate stability

Î± = 0.1 â†’ slow but smooth convergence

## ğŸ§­ Optimal policy arrows

# Each grid cell shows:

Value estimate

Arrow for best action

Special markings: S, G, J, JT, X

ğŸš€ Running the Experiment
python gridworld_q_learning.py


Dependencies:

numpy
matplotlib


Install with:

pip install numpy matplotlib

## ğŸ§© Future Enhancements

Deep Q-Learning (DQN) version

Stochastic wind / cliff variants

Multi-agent comparison

SARSA implementation

OpenAI Gym wrapper

## ğŸ“œ License

MIT License
Â© 2025 â€” Shahmeer Khan
